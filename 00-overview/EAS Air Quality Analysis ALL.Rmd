---
title: "Introduction to Air Quality Data Analysis"
#output: html_notebook
editor_options: 
  markdown: 
    wrap: 72
---

# Overview

This code gives an overview on: - Loading air quality data from QuantAQ
instruments - Performing initial summary analysis - Loading meteorology
data from public data - Combining air quality and meteorology data -
Conducting exploratory analysis on dynamics of air pollutants related to
time and meteorology

# Downloading and Importing

We're going to be using two datasets: QuantAQ pollutant concentration
data from the [QuantAQ website](https://www.quant-aq.com/) and
meteorology data from the [Iowa Environmental
Mesonet](https://mesonet.agron.iastate.edu/request/download.phtml?network=MA_ASOS).

The notes here will describe the downloading, storing and importing
process for a normal workflow.

## QuantAQ Data

Download the full time series from the sensor(s) you're interested in
from the QuantAQ website.

## Meteorology Data

Select the relevant station for syncing with your air quality data;
e.g.: \* [BOS] BOSTON/LOGAN INTL

Select the following variables:

-   Wind Direction
-   Wind Speed [mph]
-   Temperature (C)

Select the date range that matches your air quality data; e.g.: \*
9/7/2019 to 3/8/2023 (this dataset is not inclusive of the last date)

Select the relevant local timezone; e.g.: \* America/New_York

Use the following download options: \* csv \* no latitude/ longitude
vectors \* represent missing data with blank string \* denote trace with
blank string

## Storing the data

In order to easily import the data, we'll store the data in the
following way:

-   Store all the QuantAQ data in the folder called ./data/quant_aq,
    which is a subfolder in "/data" folder of this workspace.
-   Store the meteorology data in the ./data/met folder in this
    workspace.

# AIR QUALITY DATA LOADING

# Loading initial packages

```{r, warning= FALSE, error = FALSE}
# There are a few packages that we'll start using immediately and that will be used throughout the .rmd file. We'll load them first.
library(data.table) #to use the data.table variable type
library(dplyr) #this library allows you to use the %>% operator
library(tidyr) #this library lets you use the complete function to account for time syncing
library(tidyverse) #this library allows .csv reading
library(lubridate) #deals with datetime formatting
library(openair) #this package efficiently geinerates figures for air quality data analysis. it's our best friend.
library(openairmaps) #this package will allow us to superimpose openair figures onto maps later on.

```

# Load Air Quality Data file.

```{r}
# Here's where you'll define the file path for your data file and load the data.
data_file <- "./data/quant_aq/MOD-00025-Roxbury.csv"  # Data file path for AQ data
aq_df <- read_csv(data_file)  # Load the data with read_csv command (there are a bunch of ways to get this done, but this one is efficient and quick). This creates a data frame called "aq_df".


```

# Summary statistics

```{r}
# Looking briefly at summary statistics will give you a sense of the variables that you just loaded. This can also tip you off to things that might need attention. 
# What do you notice in these summary statistics?

summary(aq_df)

```

# Date formatting

```{r}
# Dates and times have a bunch of different possible formats. These formats can be the bane of your analysis existence. Thankfully the lubridate package (which we loaded above) makes things smoother. It even automatically detects daylight saving time!
# $ is used to get or create the attribute of a variable, i.e. "date" of "aq_df" var
aq_df$date <- as.POSIXct(strptime(aq_df$timestamp_local, format = "%Y-%m-%d %H:%M:%S", tz = "America/New_York"))


```

# Time series

```{r}
# We'll spend a lot of time working with the openair package in our project and will get a more in-depth introduction in the next class. But we'll draw on one of its functions now.
# As you look at this time series, what sticks out to you?
# Where do you notice points that seem outside of what is reasonable?

# OpenAir function
# timePlot(aq_df, pollutant = "co")

# c() is matrix, y.relation="free" allows different y-scales
#timePlot(aq_df, pollutant = c("co", "o3", "no", "no2", "pm1", "pm25", "pm10"), y.relation= "free")
#timePlot(aq_df, pollutant = c("co", "o3", "no", "no2"), y.relation= "free")
timePlot(aq_df, pollutant = c("pm1", "pm25", "pm10"), y.relation= "free")



```

# CLEANING STEPS

## Define an outlierReplace function now and use it later

```{r}
outlierReplace = function(dataframe, cols, rows, newValue = NA) {
    if (any(rows)) {
        set(dataframe, rows, cols, newValue)
    }
}
```

## Define threshold values

```{r}
# What are the maximum values you might reasonably expect to see in ambient air?
# How do these compare with NAAQS?
pm10_threshold <- 1000
pm25_threshold <- 100
pm1_threshold <- 50
no_threshold <- 30
no2_threshold <- 400
co_threshold <- 4000
o3_threshold <- 200
ws_threshold <- 10  # in m/s; filtered after mph to m/s conversion

```

## Filter using outlierReplace function

```{r}
# Replace values above threshold defined above, filter values below zero

aq_df_filtered<-aq_df   # Create a new dataframe that will be filtered and preserve the original

# Filter out above thresholds
outlierReplace(aq_df_filtered, "pm10", which(aq_df_filtered$pm10 > pm10_threshold), NA)
outlierReplace(aq_df_filtered, "pm25", which(aq_df_filtered$pm25 > pm25_threshold), NA)
outlierReplace(aq_df_filtered, "pm1", which(aq_df_filtered$pm1 > pm1_threshold), NA)

# outlierReplace(aq_df_filtered, "no", which(aq_df_filtered$no > no_threshold), NA)
# outlierReplace(aq_df_filtered, "no2", which(aq_df_filtered$no2 > no2_threshold), NA)
# outlierReplace(aq_df_filtered, "co", which(aq_df_filtered$co > co_threshold), NA)
# outlierReplace(aq_df_filtered, "o3", which(aq_df_filtered$o3 > o3_threshold), NA)

# Filter out below zero, since negative pollutant concentrations aren't real
outlierReplace(aq_df_filtered, "pm10", which(aq_df_filtered$pm10 < 0), NA)
outlierReplace(aq_df_filtered, "pm25", which(aq_df_filtered$pm25 < 0), NA)
outlierReplace(aq_df_filtered, "pm1", which(aq_df_filtered$pm1 < 0), NA)

# outlierReplace(aq_df_filtered, "no", which(aq_df_filtered$no < 0), NA)
# outlierReplace(aq_df_filtered, "no2", which(aq_df_filtered$no2 < 0), NA)
# outlierReplace(aq_df_filtered, "co", which(aq_df_filtered$co < 0), NA)
# outlierReplace(aq_df_filtered, "o3", which(aq_df_filtered$o3 < 0), NA)



```

## Sanity check time series - did you do your cleaning job?

```{r}
# timePlot(aq_df_filtered, pollutant = "co")

#timePlot(aq_df_filtered, pollutant = c("co", "o3", "no", "no2", "pm1", "pm25", "pm10"), scales= "free")
#timePlot(aq_df_filtered, pollutant = c("co", "o3", "no", "no2"), scales= "free")
timePlot(selectByDate(aq_df_filtered, year=2023, month=6), pollutant = c("pm1", "pm25", "pm10"), scales= "free")

```

# LOADING AND MERGING METEOROLOGY DATA

# Meteorology Data

```{r}
# Now we'll import and sync meteorology data set up analysis of air pollutant concentrations related to local meteorology.

# Set directory and file for met data
met_file <- "data/met/boston_met.csv"

# Read met data
metdata <- fread(met_file, header=TRUE, data.table = TRUE) #import meteorology file using a different method than we used above

#Deal with date/time formatting
metdata$date <- as.POSIXct(metdata$valid, format = "%Y-%m-%d %H:%M", tz = "America/New_York") #setting datetime, using correct timezone on East Coast Local time

# Rename variables to play well with Openair
# Use dplyr package to allow for the %>% operator (basically, pass this dataframe through a set of sequantial manipulations)

metdata <- metdata %>%
  setnames(old = c("drct", "sped", "valid"), new = c("wd", "ws", "original_met_time")) %>% #rename
  na.omit("date") %>% #if any dates are NA, the following function won't work
  complete(date = seq(from = min(date), to= max(date),  by = "1 min")) %>%  # make 1 minute intervals, and fill in any missing dates/times with "NA"
  fill(c("wd", "ws", "tmpc")) # fill those new 1 minute interval rows. "fill" replaces "NA" with the previous valid (non-NA) datapoint.

# Convert wind speed to m/s
metdata$ws <- metdata$ws * (1609/3600) #converting to m/s, 1609 meters per mile, 3600 seconds per hr
outlierReplace(metdata, "ws", which(metdata$ws > ws_threshold), NA)

# Remove station data
metdata[,c( "station")] <- list(NULL) #getting rid of unnecessary variables

# Remove duplicate values. They make lots of stuff messier later.
metdata <- unique(metdata) 
```

# Time sync and merge met and pollution data

## Round to get matching timestamps

```{r}
# First round datetime for each dataframe to 1 min time resolution (they're already at 1 min resolution but may have a different set of seconds)
aq_df_filtered$date_1min <- round_date(aq_df$date, unit="minute") #round date for merging
metdata$met_date_1min <- round_date(metdata$date, unit="minute") #round date for merging
```

## Join meteorology and aq data on 1 min basis

```{r}
# create new dataframe with 1 min time-synced data
aq_df_met <- left_join(aq_df_filtered, metdata, by = c("date_1min" = "met_date_1min"))

# deal with a funky naming artifact
aq_df_met$date <- aq_df_met$date.x

```

## Ensure that merging worked: summary and time series

```{r}
# Sanity check wind time series

summary(aq_df_met)

timePlot(aq_df_met, pollutant = c("ws", "wd", "tmpc", "pm1"), scales = "free")
```

# EXPLORATORY DATA ANALYSIS

## Diurnal Profiles

```{r}
# Diurnals: First look at a "typical day" for different pollutants
# Read openair manual to find different ways to use this function. Things that may be helpful: selectByDate; type (season, etc); normalise; subset

# timeVariation(aq_df_met, pollutant = c("no", "no2", "co"), local.tz= "America/New_York", normalise = TRUE) 

timeVariation(aq_df_met, pollutant = c("pm1", "pm25", "pm10"), local.tz= "America/New_York") 

# timeVariation(selectByDate(aq_df_met, month = c(1:5, 9:12)), pollutant = c('pm1', 'pm25', 'pm10'))
# myOutput <- timeVariation(aq_df_met, pollutant = "o3", statistic = "median", local.tz= "America/New_York", col = "firebrick", type = "season")#, subset = "hour") 
# plot(myOutput, subset = "hour")

```

## Directional analysis of pollutants

### Set sensor location by lat/long

```{r}
# Montreal: 45.392796,-74.062294
# Pueblo: 38.16,-104.624
# Hartford: 41.784874,-72.6315
# MOD 25: 42.32862119, -71.086748391

aq_df_met$lat <- 42.32862119
aq_df_met$long <- -71.086748391


```

### Create polar plots (and other things in that family)

```{r}
# this group of functions allows you to explore the relationship between wind speed, wind direction, and pollutant concentrations. They all give slightly different flavors of the relationship between pollutants and meteorology, so explore and see what stories emerge. 

# There are a bunch of different things you can define here - type (seasonal, etc), statistic (median, mean, max, standard deviation, etc), color scale limits, icon size, figure transparency, etc. In particular, the type and statistic inputs give you *very* different pictures of what's happening in an environment.

polarPlot(selectByDate(aq_df_met, year='2023', month=c(1, 7)), pollutant = "pm10", main="PM10 During construction (1/2023-7/2023)", limits = c(0, 40))
polarPlot(selectByDate(aq_df_met, year='2023', month=c(8, 12)), pollutant = "pm10", main="PM10 After construction (8/2023-12/2023)", limits = c(0, 40))

# polarPlot(aq_df_met, pollutant = "pm10", type = "season")
# polarPlot(aq_df_met, pollutant = "pm10", limits = c(0,8), type = "season")
# polarFreq(aq_df_met, pollutant = "pm10")
# pollutionRose(aq_df_met, pollutant = "pm10")
# polarAnnulus(aq_df_met, pollutant = "pm10")
# polarCluster(aq_df_met, pollutant = "pm10", n.clusters = 4)

aq_df_met$ratio <- (aq_df_met$no + aq_df_met$no2) / aq_df_met(co)
polarPlot(aq_df_met, pollutant = "ratio")


```

### Create Polar map plots

```{r}
#polarMap is an extension of polarPlot that places a polarPlot onto a map as an overlay. polarPlot is in a family of plots that include pollutionRose, polarAnnulus, polarFreq, percentileRose, and others that give slightly different visualizations of the same combined pollutant/met data. You should play with some of these too.

# There are a bunch of different things you can define here - type (seasonal, etc), statistic (median, mean, max, standard deviation, etc), color scale limits, icon size, figure transparency, etc. In particular, the type and statistic inputs give you *very* different pictures of what's happening in an environment.

polarMap(selectByDate(aq_df_met, year='2023', month=c(1, 7)),
         pollutant = "pm10", 
         key.position = "bottom",
         key.header = "PM10 (ug/m3)", 
         key.footer = NULL, 
         x = "ws",
         latitude = "lat",
         longitude = "long", 
         provider = "OpenStreetMap",
         limits = c(0, 40),
         cols = "jet",
         alpha = 0.8,
         key = TRUE,
         iconWidth = 200,
         iconHeight = 200,
         fig.width = 4,
         fig.height = 4
         )


polarMap(selectByDate(aq_df_met, year='2023', month=c(8, 12)),
         pollutant = "pm10", 
         key.position = "bottom",
         key.header = "PM10 (ug/m3)", 
         key.footer = NULL, 
         x = "ws",
         latitude = "lat",
         longitude = "long", 
         provider = "OpenStreetMap",
         limits = c(0, 40),
         cols = "jet",
         alpha = 0.8,
         key = TRUE,
         iconWidth = 200,
         iconHeight = 200,
         fig.width = 4,
         fig.height = 4
         )


# polarMap(aq_df_met,
#          pollutant = "no", 
#          key.position = "bottom",
#          key.header = "NO (ppb)", 
#          key.footer = NULL, 
#          x = "ws",
#          latitude = "lat",
#          longitude = "long", 
#          provider = "OpenStreetMap",
#          limits = c(0, 4),
#          cols = "jet",
#          alpha = 0.8,
#          key = TRUE,
#          iconWidth = 200,
#          iconHeight = 200,
#          fig.width = 4,
#          fig.height = 4
#          )
# 
# 
# polarMap(aq_df_met,
#          pollutant = "no", 
#          key.position = "bottom",
#          key.header = "NO (ppb)", 
#          x = "ws",
#          latitude = "lat",
#          longitude = "long", 
#          provider = "OpenStreetMap",
#          limits = c(0, 4),
#          cols = "jet",
#          key = TRUE,
#          )

```

### Create some calendar plots to explore long-term temporal trends

```{r}
# These can help identify specific dates or ranges of dates that had particularly high pollutant concentrations. You may want to look more closely at conspicuously elevated pollutant days.

calendarPlot(selectByDate(aq_df_met, year='2023'), pollutant = "pm10")
calendarPlot(selectByDate(aq_df_met, year='2023'), pollutant = "pm10", limits = c(0,70))


calendarPlot(aq_df_met, pollutant = "pm10", annotate = "value", limits = c(0,80),
lim =50, cols = "Purples", col.lim = c("black", "orange"), layout = c(4, 3))

trendLevel(aq_df_met, pollutant = "pm25", limits = c(0,30))


```
